{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc56b96a",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591fb59c",
   "metadata": {},
   "source": [
    "### HASOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d86bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589a7e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3819\n",
      "850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_de_1</td>\n",
       "      <td>Frank Rennicke – Ich bin stolz https://t.co/Cm...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_id                                               text task_1 task_2\n",
       "0  hasoc_de_1  Frank Rennicke – Ich bin stolz https://t.co/Cm...    NOT   NONE"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "train_filename = os.path.join(dirname, \"..\\\\data\\\\HateSpeechRecoginition\\\\HASOC_german_dataset\\\\german_dataset.tsv\")\n",
    "test_filename = os.path.join(dirname, '..\\\\data\\\\HateSpeechRecoginition\\\\HASOC_german_dataset\\\\hasoc_de_test_gold.tsv')\n",
    "\n",
    "\n",
    "train_raw_data = pd.read_csv(train_filename, sep='\\t')\n",
    "test_raw_data = pd.read_csv(test_filename, sep='\\t')\n",
    "\n",
    "print(len(train_raw_data))\n",
    "print(len(test_raw_data))\n",
    "\n",
    "train_raw_data.loc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "172adefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# remove all usernames\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(\"@[^\\s]+\",'USER',tweet)\n",
    "    tweet = re.sub(\"http[^\\s]+\", \"\\n\", tweet) #Remove http links\n",
    "    tweet = re.sub(\"www[^\\s]+\", \"\\n\", tweet) #Remove http links\n",
    "    tweet = emoji.replace_emoji(tweet, '') #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "train_raw_data['text'] = train_raw_data['text'].map(lambda x: clean_tweet(x))\n",
    "test_raw_data['text'] = test_raw_data['text'].map(lambda x: clean_tweet(x))\n",
    "\n",
    "# Binary labeling\n",
    "train_raw_data['task_1'] = train_raw_data['task_1'].str.replace(\"NOT\",\"0\")\n",
    "test_raw_data['task_1'] = train_raw_data['task_1'].str.replace(\"NOT\",\"0\")\n",
    "train_raw_data['task_1'] = train_raw_data['task_1'].str.replace(\"HOF\",\"1\")\n",
    "test_raw_data['task_1'] = train_raw_data['task_1'].str.replace(\"HOF\",\"1\")\n",
    "\n",
    "# Create a validation dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation = train_test_split(train_raw_data, test_size=0.05, random_state=42)\n",
    "\n",
    "# Save\n",
    "train.to_csv('../data/HateSpeechRecoginition/HASOC_train.csv')\n",
    "test_raw_data.to_csv('../data/HateSpeechRecoginition/HASOC_test.csv')\n",
    "validation.to_csv('../data/HateSpeechRecoginition/HASOC_validation.csv')\n",
    "\n",
    "print(len(validation))\n",
    "\n",
    "del train_raw_data\n",
    "del test_raw_data\n",
    "del validation\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9da8ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39229cff26d445eb8622f0a1d727585b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a97c9bbe9b14d1fae9f35f257929466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c589fa4adff4912bec41d4775894eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7684cb0b99f244e29ab753c2e2aea18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d4265bc84b4f0eb86451e66e0da3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'text_id', 'text', 'task_1', 'task_2'],\n",
       "        num_rows: 3628\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'text_id', 'text', 'task_1', 'task_2'],\n",
       "        num_rows: 850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'text_id', 'text', 'task_1', 'task_2'],\n",
       "        num_rows: 191\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open data in a suitable format for pytorch\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "dataset = load_dataset('../data/HateSpeechRecoginition', data_files={'train': 'HASOC_train.csv', 'test': 'HASOC_test.csv', 'validation': 'HASOC_validation.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315a5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = dataset[\"train\"]\n",
    "raw_test_ds = dataset[\"test\"]\n",
    "raw_val_ds = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4d3fa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f6ffb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc31c4121af4d7fa5ddd502ee4910e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71ac0aaada44b1ca0b530b5abae31ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3026d7d41f45bca53180abc73f6317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 2304,\n",
       " 'text_id': 'hasoc_de_2305',\n",
       " 'text': \"ef:'Korruption der Herrscher in Afrika: Der Griff in die Staatskasse: Korruption der Herrscher in Afrika: Der Griff in die Staatskasse \\n '\",\n",
       " 'task_1': 0,\n",
       " 'task_2': 'NONE',\n",
       " 'input_ids': [3,\n",
       "  454,\n",
       "  26913,\n",
       "  26964,\n",
       "  26979,\n",
       "  18183,\n",
       "  21,\n",
       "  14273,\n",
       "  50,\n",
       "  9061,\n",
       "  26964,\n",
       "  233,\n",
       "  11759,\n",
       "  50,\n",
       "  30,\n",
       "  1477,\n",
       "  9472,\n",
       "  26964,\n",
       "  18183,\n",
       "  21,\n",
       "  14273,\n",
       "  50,\n",
       "  9061,\n",
       "  26964,\n",
       "  233,\n",
       "  11759,\n",
       "  50,\n",
       "  30,\n",
       "  1477,\n",
       "  9472,\n",
       "  26979,\n",
       "  4],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-german-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffb3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3628\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 191\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text_id\", \"text\",\"task_2\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"task_1\", \"labels\")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ca2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63d2cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29a77b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac64c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments('test-trainer', evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "218f3006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1362' max='1362' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1362/1362 05:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.289316</td>\n",
       "      <td>0.921466</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.262126</td>\n",
       "      <td>0.921466</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.339325</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1362, training_loss=0.34073726301151225, metrics={'train_runtime': 333.0026, 'train_samples_per_second': 32.684, 'train_steps_per_second': 4.09, 'total_flos': 370287699316800.0, 'train_loss': 0.34073726301151225, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df36dcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/HateSpeechRecognition/tokenizer_02\\\\tokenizer_config.json',\n",
       " 'models/HateSpeechRecognition/tokenizer_02\\\\special_tokens_map.json',\n",
       " 'models/HateSpeechRecognition/tokenizer_02\\\\vocab.txt',\n",
       " 'models/HateSpeechRecognition/tokenizer_02\\\\added_tokens.json',\n",
       " 'models/HateSpeechRecognition/tokenizer_02\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save\n",
    "\n",
    "model.save_pretrained('../models/HateSpeechRecognition/model_02')\n",
    "tokenizer.save_pretrained('../models/HateSpeechRecognition/tokenizer_02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82822428",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5442288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 2) (850,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd86023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9105882352941177, 'f1': 0.05}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e82432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 38, 'labels': 1, 'input_ids': [3, 3011, 5477, 26964, 26979, 1232, 1138, 287, 987, 21846, 26903, 26935, 573, 8174, 8425, 26914, 4847, 21846, 26903, 26935, 9593, 142, 30, 2586, 2579, 16391, 26914, 892, 6378, 18725, 6123, 232, 30, 21846, 26903, 26935, 7408, 7286, 281, 42, 287, 3361, 311, 6690, 142, 380, 8801, 21012, 26914, 2, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"test\"][38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69b7f024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1][38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds, columns=[\"predictions\"])\n",
    "df['labels'] = predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab40224f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictions  labels\n",
       "0              1       0\n",
       "1              0       0\n",
       "2              0       0\n",
       "3              0       0\n",
       "4              0       0\n",
       "..           ...     ...\n",
       "845            0       0\n",
       "846            0       0\n",
       "847            0       0\n",
       "848            0       0\n",
       "849            0       0\n",
       "\n",
       "[850 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99bff0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('../data/HateSpeechRecoginition/HASOC_test.csv')\n",
    "df['text'] = test_dataset['text']\n",
    "df.to_csv('../data/HateSpeechRecoginition/test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b7edfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a760312fc6f4e6e94b19bb8266cf89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104dca9cb02747ac98b84db232942acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3a535335ca46efbba026030d1e670c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating bestande_en split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bestande_dataset = load_dataset('../data', data_files={'bestande_de': 'reviews_de.csv'})\n",
    "\n",
    "## First some preprocessing is needed for the translated file\n",
    "translations = pd.read_csv('../data/reviews_en_translated.csv')\n",
    "translations['translations'].replace('', np.nan, inplace=True)\n",
    "translations.dropna(subset=['translations'], inplace=True)\n",
    "\n",
    "def clean_review(review):\n",
    "    review = re.sub(\"http[^\\s]+\", \"\\n\", review) #Remove http links\n",
    "    review = re.sub(\"www[^\\s]+\", \"\\n\", review) #Remove www links\n",
    "    review = emoji.replace_emoji(review, '') #Remove Emojis\n",
    "    review = review.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    \n",
    "    return review\n",
    "\n",
    "translations['translations'] = translations['translations'].map(lambda x: clean_review(x))\n",
    "translations.to_csv('../data/HateSpeechRecoginition/bestande_translations_clean.csv')\n",
    "\n",
    "bestande_dataset_en = load_dataset('../data/HateSpeechRecoginition', data_files={'bestande_en': 'bestande_translations_clean.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edf1aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestande_dataset = bestande_dataset.remove_columns([\"score\", \"university\",\"course\", \"upvotes\", \"downvotes\",\"date\",\"courseName\",\"courseNameShort\"])\n",
    "bestande_dataset_en = bestande_dataset_en.remove_columns([\"review\", \"score\", \"university\",\"course\", \"upvotes\", \"downvotes\",\"date\",\"courseName\",\"courseNameShort\"])\n",
    "bestande_dataset_en = bestande_dataset_en.rename_column(\"translations\", \"review\")\n",
    "bestande_dataset_en = bestande_dataset_en.remove_columns([\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ed12f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    bestande_de: Dataset({\n",
       "        features: ['_id', 'review'],\n",
       "        num_rows: 6642\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestande_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "befa36db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    bestande_en: Dataset({\n",
       "        features: ['Unnamed: 0', '_id', 'review'],\n",
       "        num_rows: 2840\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestande_dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05e98c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'review', 'Unnamed: 0'],\n",
       "    num_rows: 9482\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "bestande_dataset = concatenate_datasets([bestande_dataset[\"bestande_de\"], bestande_dataset_en[\"bestande_en\"]])\n",
    "bestande_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a7b75ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620bc490467b4e809f5c78deff64716c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function_bestande(example):    \n",
    "    return tokenizer(example[\"review\"], truncation=True)\n",
    "\n",
    "tokenized_bestande = bestande_dataset.map(tokenize_function_bestande, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c91a6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'review', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 9482\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_bestande.remove_columns(['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca17ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_bestande = trainer.predict(tokenized_bestande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c8ff810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9482, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_bestande.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54644ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 2.5105774, -2.2286   ],\n",
       "       [ 2.4784086, -2.1966133],\n",
       "       [ 2.3761032, -2.0908227],\n",
       "       ...,\n",
       "       [ 2.5498087, -2.263361 ],\n",
       "       [ 2.1101553, -1.8095356],\n",
       "       [ 2.5377057, -2.2536256]], dtype=float32), label_ids=None, metrics={'test_runtime': 133.1739, 'test_samples_per_second': 71.2, 'test_steps_per_second': 8.906})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_bestande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f359ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions_bestande.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25026b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86e4eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bestande = pd.DataFrame(tokenized_bestande['_id'], columns=[\"id\"])\n",
    "df_bestande['reviews'] = tokenized_bestande['review']\n",
    "df_bestande['prediction'] = preds\n",
    "df_bestande.to_csv('../data/HateSpeechRecoginition/bestande_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21cc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

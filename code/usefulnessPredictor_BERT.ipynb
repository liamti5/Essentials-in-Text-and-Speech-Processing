{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c721447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import EarlyStoppingCallback, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092d7c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', '_id', 'review', 'score', 'upvotes', 'downvotes', 'sum'],\n",
       "        num_rows: 7656\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', '_id', 'review', 'score', 'upvotes', 'downvotes', 'sum'],\n",
       "        num_rows: 1423\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', '_id', 'review', 'score', 'upvotes', 'downvotes', 'sum'],\n",
       "        num_rows: 403\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('../data/ReviewPrediction', data_files={'train': 'train.csv', 'test': 'test.csv', 'validation': 'validation.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94256c",
   "metadata": {},
   "source": [
    "## BERT base german cased - transformers trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84925914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 2347,\n",
       " '_id': '61e0642e1875bb0021077478',\n",
       " 'review': 'Alles irreführend mit diesem Kurs: Prüfungsrelevante Inhalte, Prüfungsfragen, Aufsicht oder keine Aufsicht, Dias und vieles mehr!',\n",
       " 'score': 1,\n",
       " 'upvotes': 0.0,\n",
       " 'downvotes': 0.0,\n",
       " 'sum': 0,\n",
       " 'input_ids': [3,\n",
       "  10795,\n",
       "  18140,\n",
       "  24387,\n",
       "  114,\n",
       "  798,\n",
       "  6436,\n",
       "  26964,\n",
       "  10575,\n",
       "  18017,\n",
       "  26,\n",
       "  13940,\n",
       "  26918,\n",
       "  10575,\n",
       "  5521,\n",
       "  26918,\n",
       "  12671,\n",
       "  309,\n",
       "  668,\n",
       "  12671,\n",
       "  26918,\n",
       "  1824,\n",
       "  45,\n",
       "  42,\n",
       "  17422,\n",
       "  380,\n",
       "  26982,\n",
       "  4],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 'bert-base-german-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['review'], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bfd949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 7656\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1423\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 403\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"upvotes\", \"downvotes\", \"score\", \"Unnamed: 0\", '_id', 'review'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"sum\", \"labels\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427f756a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8195a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc28043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    results = {}\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    class_report= classification_report(y_pred=predictions, y_true=labels)\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    print(\"############ Classification report ############\")\n",
    "    print(class_report)\n",
    "    print(\"############      Accuracy       ##############\")\n",
    "    print(acc)\n",
    "    results.update({'classification report' : class_report})\n",
    "    results.update({'accuracy':acc})\n",
    "    return results\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    'test-trainer', \n",
    "    auto_find_batch_size=True,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    num_train_epochs=3,\n",
    "    #per_device_train_batch_size=4,  \n",
    "    #per_device_eval_batch_size=1,\n",
    "    #eval_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    #save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd97d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='2871' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  23/2871 00:07 < 17:32, 2.71 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='5742' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 130/5742 00:22 < 16:39, 5.62 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11484' max='11484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11484/11484 25:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Classification report</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.324200</td>\n",
       "      <td>1.442410</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.70      1.00      0.83       283\n",
       "           1       0.00      0.00      0.00        99\n",
       "           2       0.00      0.00      0.00        12\n",
       "           3       0.00      0.00      0.00         9\n",
       "\n",
       "    accuracy                           0.70       403\n",
       "   macro avg       0.18      0.25      0.21       403\n",
       "weighted avg       0.49      0.70      0.58       403\n",
       "</td>\n",
       "      <td>0.702233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.247000</td>\n",
       "      <td>1.990767</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.71      0.95      0.81       283\n",
       "           1       0.17      0.04      0.07        99\n",
       "           2       0.00      0.00      0.00        12\n",
       "           3       0.50      0.11      0.18         9\n",
       "\n",
       "    accuracy                           0.68       403\n",
       "   macro avg       0.35      0.27      0.26       403\n",
       "weighted avg       0.55      0.68      0.59       403\n",
       "</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.120800</td>\n",
       "      <td>2.204541</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.73      0.88      0.80       283\n",
       "           1       0.33      0.19      0.24        99\n",
       "           2       0.00      0.00      0.00        12\n",
       "           3       0.33      0.11      0.17         9\n",
       "\n",
       "    accuracy                           0.67       403\n",
       "   macro avg       0.35      0.30      0.30       403\n",
       "weighted avg       0.60      0.67      0.62       403\n",
       "</td>\n",
       "      <td>0.667494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Classification report ############\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83       283\n",
      "           1       0.00      0.00      0.00        99\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.70       403\n",
      "   macro avg       0.18      0.25      0.21       403\n",
      "weighted avg       0.49      0.70      0.58       403\n",
      "\n",
      "############      Accuracy       ##############\n",
      "0.7022332506203474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Classification report ############\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.81       283\n",
      "           1       0.17      0.04      0.07        99\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.50      0.11      0.18         9\n",
      "\n",
      "    accuracy                           0.68       403\n",
      "   macro avg       0.35      0.27      0.26       403\n",
      "weighted avg       0.55      0.68      0.59       403\n",
      "\n",
      "############      Accuracy       ##############\n",
      "0.6774193548387096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Classification report ############\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       283\n",
      "           1       0.33      0.19      0.24        99\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.33      0.11      0.17         9\n",
      "\n",
      "    accuracy                           0.67       403\n",
      "   macro avg       0.35      0.30      0.30       403\n",
      "weighted avg       0.60      0.67      0.62       403\n",
      "\n",
      "############      Accuracy       ##############\n",
      "0.6674937965260546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11484, training_loss=1.255251861540377, metrics={'train_runtime': 1507.9243, 'train_samples_per_second': 15.232, 'train_steps_per_second': 7.616, 'total_flos': 755824745430144.0, 'train_loss': 1.255251861540377, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e329ae",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization  - BERT base german cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dcfc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "def optuna_hp_space(trial):\n",
    "    return{\n",
    "        \"learning_rate\" : trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64, 128])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8f00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial: optuna.Trial):     \n",
    "    model,\n",
    "    training_args = TrainingArguments(         \n",
    "        output_dir='./hp_optimization',\n",
    "        learning_rate=trial.suggest_float('learning_rate', low=1e-6, high=0.01),         \n",
    "        weight_decay=trial.suggest_float('weight_decay', 4e-5, 0.01),         \n",
    "        num_train_epochs=trial.suggest_int('num_train_epochs', low=2,high=8),         \n",
    "        #per_device_train_batch_size=trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64, 128]),         \n",
    "        #per_device_eval_batch_size=trial.suggest_categorical(\"per_device_eval_batch_size\", [4, 8, 16, 32, 64, 128]),         \n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        disable_tqdm=True)     \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        #compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    result = trainer.train()     \n",
    "    return result.training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7109cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 16:35:51,981] A new study created in memory with name: hp-search-electra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggering Optuna study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[W 2023-10-17 16:36:06,293] Trial 0 failed with parameters: {'learning_rate': 0.008665374934488372, 'weight_decay': 0.005422559322396213, 'num_train_epochs': 4} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 3.22 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_34152\\3044404180.py\", line 26, in objective\n",
      "    result = trainer.train()\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py\", line 1556, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py\", line 1838, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py\", line 2693, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py\", line 2718, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1562, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1022, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 612, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 497, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 427, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 359, in forward\n",
      "    attention_probs = self.dropout(attention_probs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\", line 59, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\functional.py\", line 1252, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 3.22 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-10-17 16:36:06,299] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 3.22 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTriggering Optuna study\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhp-search-electra\u001b[39m\u001b[38;5;124m'\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m----> 7\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(         \n\u001b[0;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./hp_optimization\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),         \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     14\u001b[0m     disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)     \n\u001b[0;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtraining_loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1554\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1838\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1841\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1843\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1844\u001b[0m ):\n\u001b[0;32m   1845\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1846\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2693\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2696\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py:2718\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2717\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2718\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2719\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2720\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:359\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 3.22 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#                    CREATE OPTUNA STUDY\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Triggering Optuna study')\n",
    "study = optuna.create_study(study_name='hp-search-electra', direction='minimize') \n",
    "study.optimize(func=objective, n_trials=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e10ec",
   "metadata": {},
   "source": [
    "## Weighted loss function trainer - BERT german cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab03e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with weight balanced loss\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.03, 0.08, 0.39, 0.5], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    results = {}\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    class_report= classification_report(y_pred=predictions, y_true=labels)\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    print(\"############ Classification report ############\")\n",
    "    print(class_report)\n",
    "    print(\"############      Accuracy       ##############\")\n",
    "    print(acc)\n",
    "    results.update({'classification report' : class_report})\n",
    "    results.update({'accuracy':acc})\n",
    "    return results\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    'test-trainer', \n",
    "    auto_find_batch_size=True,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    num_train_epochs=3,\n",
    "    #per_device_train_batch_size=4,  \n",
    "    #per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = CustomLossTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5409af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/ReviewPrediction/tokenizer_01_01_68\\\\tokenizer_config.json',\n",
       " '../models/ReviewPrediction/tokenizer_01_01_68\\\\special_tokens_map.json',\n",
       " '../models/ReviewPrediction/tokenizer_01_01_68\\\\vocab.txt',\n",
       " '../models/ReviewPrediction/tokenizer_01_01_68\\\\added_tokens.json',\n",
       " '../models/ReviewPrediction/tokenizer_01_01_68\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../models/ReviewPrediction/model_01_01_68')\n",
    "tokenizer.save_pretrained('../models/ReviewPrediction/tokenizer_01_01_68')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b05321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Classification report ############\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.81       283\n",
      "           1       0.32      0.16      0.21        99\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.40      0.22      0.29         9\n",
      "\n",
      "    accuracy                           0.68       403\n",
      "   macro avg       0.36      0.32      0.33       403\n",
      "weighted avg       0.60      0.68      0.63       403\n",
      "\n",
      "############      Accuracy       ##############\n",
      "0.6774193548387096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 4) (403,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6594f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6774193548387096}\n",
      "{'precision': 0.36318965517241375}\n",
      "{'recall': 0.3212246136274405}\n",
      "{'f1': 0.32718006846640424}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "print(accuracy_metric.compute(predictions=preds, references=predictions.label_ids))\n",
    "print(precision_metric.compute(predictions=preds, references=predictions.label_ids, average='macro'))\n",
    "print(recall_metric.compute(predictions=preds, references=predictions.label_ids, average='macro'))\n",
    "print(f1_metric.compute(predictions=preds, references=predictions.label_ids, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed743ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       3, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bf3af",
   "metadata": {},
   "source": [
    "## BERT base multilingual uncased sentiment - fine tuning attempt in Colab due to OOM exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aaf9d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 2347,\n",
       " '_id': '61e0642e1875bb0021077478',\n",
       " 'review': 'Alles irreführend mit diesem Kurs: Prüfungsrelevante Inhalte, Prüfungsfragen, Aufsicht oder keine Aufsicht, Dias und vieles mehr!',\n",
       " 'score': 1,\n",
       " 'upvotes': 0.0,\n",
       " 'downvotes': 0.0,\n",
       " 'sum': 0,\n",
       " 'input_ids': [101,\n",
       "  21785,\n",
       "  10544,\n",
       "  61581,\n",
       "  19544,\n",
       "  56772,\n",
       "  10234,\n",
       "  14296,\n",
       "  33666,\n",
       "  131,\n",
       "  14853,\n",
       "  77644,\n",
       "  37696,\n",
       "  12832,\n",
       "  33248,\n",
       "  10111,\n",
       "  95942,\n",
       "  10111,\n",
       "  117,\n",
       "  14853,\n",
       "  77644,\n",
       "  45220,\n",
       "  41533,\n",
       "  117,\n",
       "  10350,\n",
       "  47707,\n",
       "  10843,\n",
       "  14819,\n",
       "  10350,\n",
       "  47707,\n",
       "  117,\n",
       "  14347,\n",
       "  10138,\n",
       "  16994,\n",
       "  10107,\n",
       "  12600,\n",
       "  106,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['review'], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3156ce19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 7656\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1423\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 403\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"upvotes\", \"downvotes\", \"score\", \"Unnamed: 0\", '_id', 'review'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"sum\", \"labels\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417c4df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "411e0748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=4, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e42b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    results = {}\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    class_report= classification_report(y_pred=predictions, y_true=labels)\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    print(\"############ Classification report ############\")\n",
    "    print(class_report)\n",
    "    print(\"############      Accuracy       ##############\")\n",
    "    print(acc)\n",
    "    results.update({'classification report' : class_report})\n",
    "    results.update({'accuracy':acc})\n",
    "    return results\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    'test-trainer', \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    auto_find_batch_size=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb536190",
   "metadata": {},
   "source": [
    "## Load local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a734ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1423\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset('../data/ReviewPrediction', data_files={'test': 'test.csv'})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/ReviewPrediction/tokenizer_01_01_68\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/ReviewPrediction/model_01_01_68\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"review\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"upvotes\", \"downvotes\", \"score\", \"Unnamed: 0\", '_id', 'review'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"sum\", \"labels\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b92a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e142b5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6106816584680252}\n",
      "{'f1': 0.24745731465755627}\n"
     ]
    }
   ],
   "source": [
    "# Dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k:v.to(model.device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "    metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "    f1_metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "    \n",
    "print(metric.compute())\n",
    "print(f1_metric.compute(average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e13eaab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x224d1ffffd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f4e283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db524f677d834495997c7eeed6872b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccd1d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a5b98c344c458bbfdc7aecd89942ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jorgeortizv/reviewUsefulness-multiclassClassification/commit/84db749369fe0ac259384e28fd3a3c574a4ae497', commit_message='Upload BertForSequenceClassification', commit_description='', oid='84db749369fe0ac259384e28fd3a3c574a4ae497', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"jorgeortizv/reviewUsefulness-multiclassClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d8ecc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jorgeortizv/reviewUsefulness-multiclassClassification/commit/68b5f63f1cd995aef3ef476b714a1ffb40df23d8', commit_message='Upload tokenizer', commit_description='', oid='68b5f63f1cd995aef3ef476b714a1ffb40df23d8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"jorgeortizv/reviewUsefulness-multiclassClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f196838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

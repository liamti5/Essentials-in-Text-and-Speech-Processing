{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b024c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142635d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', '_id', 'review', 'score', 'upvotes', 'downvotes', 'sum'],\n",
       "        num_rows: 6826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', '_id', 'review', 'score', 'upvotes', 'downvotes', 'sum'],\n",
       "        num_rows: 1897\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', '_id', 'review', 'score', 'upvotes', 'downvotes', 'sum'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('../data/ReviewPrediction', data_files={'train': 'train_binary.csv', 'test': 'test_binary.csv', 'validation': 'validation_binary.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a877da18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0.1': 1893,\n",
       " 'Unnamed: 0': 1893,\n",
       " '_id': '5c34e1c593ac7c001ca22f47',\n",
       " 'review': 'Sehr gute Vorlesung - sehr unfaire Pr√ºfung....',\n",
       " 'score': 3,\n",
       " 'upvotes': 11.0,\n",
       " 'downvotes': 2.0,\n",
       " 'sum': 1,\n",
       " 'input_ids': [3,\n",
       "  19386,\n",
       "  4493,\n",
       "  15428,\n",
       "  27,\n",
       "  26935,\n",
       "  1120,\n",
       "  174,\n",
       "  8716,\n",
       "  942,\n",
       "  4185,\n",
       "  26914,\n",
       "  26914,\n",
       "  26914,\n",
       "  26914,\n",
       "  4],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 'bert-base-german-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['review'], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3423e506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1897\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"upvotes\", \"downvotes\", \"score\", \"Unnamed: 0\", '_id', 'review'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"sum\", \"labels\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328150d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b824111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81ce7f",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ef750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch import nn\n",
    "import optuna\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    results = {}\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    print(\"############      Accuracy       ##############\")\n",
    "    print(acc)\n",
    "    results.update({'accuracy':acc})\n",
    "    return results\n",
    "\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.35, 0.65], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    model,\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./hp_optimization',\n",
    "        learning_rate=trial.suggest_float('learning_rate', low=1e-6, high=0.01),\n",
    "        #weight_decay=trial.suggest_float('weight_decay', 4e-5, 0.01),\n",
    "        num_train_epochs=3,\n",
    "        auto_find_batch_size=True,\n",
    "        #per_device_train_batch_size=trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16]),\n",
    "        #per_device_eval_batch_size=trial.suggest_categorical(\"per_device_eval_batch_size\", [2, 4, 8, 16]),\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        #load_best_model_at_end=True,\n",
    "        #per_device_train_batch_size=8,\n",
    "        #per_device_eval_batch_size=8,\n",
    "        disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "    trainer = CustomLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        #callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    result = trainer.train()\n",
    "    return result.training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7dcdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 11:57:26,104] A new study created in memory with name: hp-search-electra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggering Optuna study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7953, 'learning_rate': 0.00932065478665541, 'epoch': 0.15}\n",
      "{'loss': 2.2478, 'learning_rate': 0.00884213261874005, 'epoch': 0.29}\n",
      "{'loss': 2.7576, 'learning_rate': 0.008363610450824689, 'epoch': 0.44}\n",
      "{'loss': 2.6335, 'learning_rate': 0.007885088282909326, 'epoch': 0.59}\n",
      "{'loss': 3.2224, 'learning_rate': 0.007406566114993966, 'epoch': 0.73}\n",
      "{'loss': 3.0855, 'learning_rate': 0.006928043947078604, 'epoch': 0.88}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.3425559947299078}\n",
      "{'eval_loss': 2.093769073486328, 'eval_accuracy': {'accuracy': 0.3425559947299078}, 'eval_runtime': 11.9509, 'eval_samples_per_second': 63.51, 'eval_steps_per_second': 7.949, 'epoch': 1.0}\n",
      "{'loss': 2.8603, 'learning_rate': 0.006449521779163243, 'epoch': 1.03}\n",
      "{'loss': 2.3825, 'learning_rate': 0.005970999611247881, 'epoch': 1.17}\n",
      "{'loss': 2.7312, 'learning_rate': 0.005492477443332519, 'epoch': 1.32}\n",
      "{'loss': 2.3674, 'learning_rate': 0.005013955275417158, 'epoch': 1.46}\n",
      "{'loss': 2.3168, 'learning_rate': 0.004535433107501797, 'epoch': 1.61}\n",
      "{'loss': 2.096, 'learning_rate': 0.004056910939586435, 'epoch': 1.76}\n",
      "{'loss': 2.5577, 'learning_rate': 0.003578388771671073, 'epoch': 1.9}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.6995222568511963, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.941, 'eval_samples_per_second': 63.563, 'eval_steps_per_second': 7.956, 'epoch': 2.0}\n",
      "{'loss': 1.9882, 'learning_rate': 0.003099866603755712, 'epoch': 2.05}\n",
      "{'loss': 1.8791, 'learning_rate': 0.0026213444358403506, 'epoch': 2.2}\n",
      "{'loss': 1.6338, 'learning_rate': 0.002142822267924989, 'epoch': 2.34}\n",
      "{'loss': 1.458, 'learning_rate': 0.0016643001000096272, 'epoch': 2.49}\n",
      "{'loss': 1.4602, 'learning_rate': 0.0011857779320942658, 'epoch': 2.64}\n",
      "{'loss': 1.3257, 'learning_rate': 0.0007072557641789043, 'epoch': 2.78}\n",
      "{'loss': 1.2673, 'learning_rate': 0.00022873359626354278, 'epoch': 2.93}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.8511493802070618, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.972, 'eval_samples_per_second': 63.398, 'eval_steps_per_second': 7.935, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 12:18:52,570] Trial 0 finished with value: 2.2721330709948253 and parameters: {'learning_rate': 0.009799176954570773}. Best is trial 0 with value: 2.2721330709948253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1233.9902, 'train_samples_per_second': 16.595, 'train_steps_per_second': 8.297, 'train_loss': 2.2721330709948253, 'epoch': 3.0}\n",
      "{'loss': 1.2526, 'learning_rate': 0.00042385381855871665, 'epoch': 0.15}\n",
      "{'loss': 1.1327, 'learning_rate': 0.0004020931748294469, 'epoch': 0.29}\n",
      "{'loss': 1.0902, 'learning_rate': 0.0003803325311001771, 'epoch': 0.44}\n",
      "{'loss': 1.1201, 'learning_rate': 0.0003585718873709073, 'epoch': 0.59}\n",
      "{'loss': 1.1038, 'learning_rate': 0.0003368112436416376, 'epoch': 0.73}\n",
      "{'loss': 1.1221, 'learning_rate': 0.00031505059991236776, 'epoch': 0.88}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.7058058977127075, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.948, 'eval_samples_per_second': 63.525, 'eval_steps_per_second': 7.951, 'epoch': 1.0}\n",
      "{'loss': 1.0017, 'learning_rate': 0.00029328995618309805, 'epoch': 1.03}\n",
      "{'loss': 1.1307, 'learning_rate': 0.00027152931245382823, 'epoch': 1.17}\n",
      "{'loss': 1.122, 'learning_rate': 0.00024976866872455846, 'epoch': 1.32}\n",
      "{'loss': 1.0881, 'learning_rate': 0.00022800802499528872, 'epoch': 1.46}\n",
      "{'loss': 1.1299, 'learning_rate': 0.00020624738126601892, 'epoch': 1.61}\n",
      "{'loss': 1.0108, 'learning_rate': 0.00018448673753674916, 'epoch': 1.76}\n",
      "{'loss': 1.0505, 'learning_rate': 0.00016272609380747936, 'epoch': 1.9}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.8937377333641052, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.961, 'eval_samples_per_second': 63.456, 'eval_steps_per_second': 7.942, 'epoch': 2.0}\n",
      "{'loss': 0.99, 'learning_rate': 0.0001409654500782096, 'epoch': 2.05}\n",
      "{'loss': 0.9617, 'learning_rate': 0.00011920480634893983, 'epoch': 2.2}\n",
      "{'loss': 0.9431, 'learning_rate': 9.744416261967005e-05, 'epoch': 2.34}\n",
      "{'loss': 1.02, 'learning_rate': 7.568351889040027e-05, 'epoch': 2.49}\n",
      "{'loss': 0.9331, 'learning_rate': 5.39228751611305e-05, 'epoch': 2.64}\n",
      "{'loss': 0.917, 'learning_rate': 3.2162231431860724e-05, 'epoch': 2.78}\n",
      "{'loss': 0.8973, 'learning_rate': 1.0401587702590951e-05, 'epoch': 2.93}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.8472688794136047, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.9681, 'eval_samples_per_second': 63.418, 'eval_steps_per_second': 7.938, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 12:40:08,709] Trial 1 finished with value: 1.0484886791617833 and parameters: {'learning_rate': 0.0004456144622879864}. Best is trial 1 with value: 1.0484886791617833.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1226.1313, 'train_samples_per_second': 16.701, 'train_steps_per_second': 8.351, 'train_loss': 1.0484886791617833, 'epoch': 3.0}\n",
      "{'loss': 1.5521, 'learning_rate': 0.002730651317132612, 'epoch': 0.15}\n",
      "{'loss': 1.6969, 'learning_rate': 0.002590459751410638, 'epoch': 0.29}\n",
      "{'loss': 1.6828, 'learning_rate': 0.0024502681856886635, 'epoch': 0.44}\n",
      "{'loss': 1.678, 'learning_rate': 0.0023100766199666894, 'epoch': 0.59}\n",
      "{'loss': 1.4225, 'learning_rate': 0.0021698850542447154, 'epoch': 0.73}\n",
      "{'loss': 1.5851, 'learning_rate': 0.0020296934885227413, 'epoch': 0.88}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 1.1877193450927734, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.973, 'eval_samples_per_second': 63.393, 'eval_steps_per_second': 7.935, 'epoch': 1.0}\n",
      "{'loss': 1.4175, 'learning_rate': 0.0018895019228007675, 'epoch': 1.03}\n",
      "{'loss': 1.3755, 'learning_rate': 0.0017493103570787932, 'epoch': 1.17}\n",
      "{'loss': 1.4033, 'learning_rate': 0.001609118791356819, 'epoch': 1.32}\n",
      "{'loss': 1.4725, 'learning_rate': 0.0014689272256348449, 'epoch': 1.46}\n",
      "{'loss': 1.4393, 'learning_rate': 0.0013287356599128708, 'epoch': 1.61}\n",
      "{'loss': 1.2685, 'learning_rate': 0.0011885440941908968, 'epoch': 1.76}\n",
      "{'loss': 1.338, 'learning_rate': 0.0010483525284689225, 'epoch': 1.9}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.3425559947299078}\n",
      "{'eval_loss': 0.8759973049163818, 'eval_accuracy': {'accuracy': 0.3425559947299078}, 'eval_runtime': 11.971, 'eval_samples_per_second': 63.403, 'eval_steps_per_second': 7.936, 'epoch': 2.0}\n",
      "{'loss': 1.2019, 'learning_rate': 0.0009081609627469483, 'epoch': 2.05}\n",
      "{'loss': 1.0289, 'learning_rate': 0.0007679693970249743, 'epoch': 2.2}\n",
      "{'loss': 1.0615, 'learning_rate': 0.0006277778313030001, 'epoch': 2.34}\n",
      "{'loss': 1.1195, 'learning_rate': 0.00048758626558102595, 'epoch': 2.49}\n",
      "{'loss': 0.9726, 'learning_rate': 0.0003473946998590519, 'epoch': 2.64}\n",
      "{'loss': 0.981, 'learning_rate': 0.00020720313413707776, 'epoch': 2.78}\n",
      "{'loss': 0.9774, 'learning_rate': 6.701156841510363e-05, 'epoch': 2.93}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.7591994404792786, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 11.975, 'eval_samples_per_second': 63.382, 'eval_steps_per_second': 7.933, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 13:01:30,307] Trial 2 finished with value: 1.3229899949912902 and parameters: {'learning_rate': 0.002870842882854586}. Best is trial 1 with value: 1.0484886791617833.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1231.4177, 'train_samples_per_second': 16.63, 'train_steps_per_second': 8.315, 'train_loss': 1.3229899949912902, 'epoch': 3.0}\n",
      "{'loss': 1.3011, 'learning_rate': 0.0018826206069186442, 'epoch': 0.15}\n",
      "{'loss': 1.3247, 'learning_rate': 0.0017859669152193607, 'epoch': 0.29}\n",
      "{'loss': 1.3689, 'learning_rate': 0.0016893132235200772, 'epoch': 0.44}\n",
      "{'loss': 1.3606, 'learning_rate': 0.0015926595318207935, 'epoch': 0.59}\n",
      "{'loss': 1.1644, 'learning_rate': 0.0014960058401215102, 'epoch': 0.73}\n",
      "{'loss': 1.3458, 'learning_rate': 0.0013993521484222267, 'epoch': 0.88}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.6574440052700923}\n",
      "{'eval_loss': 0.8736157417297363, 'eval_accuracy': {'accuracy': 0.6574440052700923}, 'eval_runtime': 12.0034, 'eval_samples_per_second': 63.232, 'eval_steps_per_second': 7.914, 'epoch': 1.0}\n",
      "{'loss': 1.1068, 'learning_rate': 0.0013026984567229432, 'epoch': 1.03}\n",
      "{'loss': 1.1736, 'learning_rate': 0.0012060447650236597, 'epoch': 1.17}\n",
      "{'loss': 1.2504, 'learning_rate': 0.001109391073324376, 'epoch': 1.32}\n",
      "{'loss': 1.2242, 'learning_rate': 0.0010127373816250927, 'epoch': 1.46}\n",
      "{'loss': 1.2621, 'learning_rate': 0.0009160836899258091, 'epoch': 1.61}\n",
      "{'loss': 1.0987, 'learning_rate': 0.0008194299982265256, 'epoch': 1.76}\n",
      "{'loss': 1.1605, 'learning_rate': 0.000722776306527242, 'epoch': 1.9}\n",
      "############      Accuracy       ##############\n",
      "{'accuracy': 0.3425559947299078}\n",
      "{'eval_loss': 0.6956579685211182, 'eval_accuracy': {'accuracy': 0.3425559947299078}, 'eval_runtime': 11.977, 'eval_samples_per_second': 63.372, 'eval_steps_per_second': 7.932, 'epoch': 2.0}\n",
      "{'loss': 1.0978, 'learning_rate': 0.0006261226148279586, 'epoch': 2.05}\n",
      "{'loss': 0.9473, 'learning_rate': 0.0005294689231286751, 'epoch': 2.2}\n",
      "{'loss': 1.0161, 'learning_rate': 0.00043281523142939157, 'epoch': 2.34}\n",
      "{'loss': 1.081, 'learning_rate': 0.000336161539730108, 'epoch': 2.49}\n",
      "{'loss': 0.9026, 'learning_rate': 0.00023950784803082454, 'epoch': 2.64}\n",
      "{'loss': 0.8999, 'learning_rate': 0.00014285415633154104, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-10-19 13:22:05,048] Trial 3 failed with parameters: {'learning_rate': 0.0019792742986179277} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_18880\\1227325410.py\", line 58, in objective\n",
      "    result = trainer.train()\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py\", line 1556, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\memory.py\", line 136, in decorator\n",
      "    return function(batch_size, *args, **kwargs)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py\", line 1816, in _inner_training_loop\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\data_loader.py\", line 393, in __iter__\n",
      "    current_batch = send_to_device(current_batch, self.device)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 160, in send_to_device\n",
      "    {\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 161, in <dictcomp>\n",
      "    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 167, in send_to_device\n",
      "    return tensor.to(device, non_blocking=non_blocking)\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-19 13:22:05,058] Trial 3 failed with value None.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#                    CREATE OPTUNA STUDY\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Triggering Optuna study')\n",
    "study = optuna.create_study(study_name='hp-search-electra', direction='minimize')\n",
    "study.optimize(func=objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f4f6a",
   "metadata": {},
   "source": [
    "## Weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc55acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.35, 0.65], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "def compute_metrics(eval_preds):\n",
    "    results = {}\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    class_report= classification_report(y_pred=predictions, y_true=labels)\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    print(\"############ Classification report ############\")\n",
    "    print(class_report)\n",
    "    print(\"############      Accuracy       ##############\")\n",
    "    print(acc)\n",
    "    results.update({'classification report' : class_report})\n",
    "    results.update({'accuracy':acc})\n",
    "    return results\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    'test-trainer', \n",
    "    auto_find_batch_size=True,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    num_train_epochs=3,\n",
    "    #per_device_train_batch_size=4,  \n",
    "    #per_device_eval_batch_size=1,\n",
    "    #eval_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = CustomLossTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4c06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/2562 00:03 < 2:31:21, 0.28 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='264' max='5121' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 264/5121 00:48 < 14:51, 5.45 it/s, Epoch 0.15/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4001' max='10239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4001/10239 08:56 < 13:57, 7.45 it/s, Epoch 1.17/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Classification report</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.745800</td>\n",
       "      <td>0.738326</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.70      0.83      0.76       499\n",
       "           1       0.50      0.32      0.39       260\n",
       "\n",
       "    accuracy                           0.66       759\n",
       "   macro avg       0.60      0.58      0.58       759\n",
       "weighted avg       0.63      0.66      0.64       759\n",
       "</td>\n",
       "      <td>0.657444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Classification report ############\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.83      0.76       499\n",
      "           1       0.50      0.32      0.39       260\n",
      "\n",
      "    accuracy                           0.66       759\n",
      "   macro avg       0.60      0.58      0.58       759\n",
      "weighted avg       0.63      0.66      0.64       759\n",
      "\n",
      "############      Accuracy       ##############\n",
      "0.6574440052700923\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e035279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Classification report ############\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      1247\n",
      "           1       0.45      0.31      0.37       650\n",
      "\n",
      "    accuracy                           0.63      1897\n",
      "   macro avg       0.57      0.56      0.55      1897\n",
      "weighted avg       0.61      0.63      0.61      1897\n",
      "\n",
      "############      Accuracy       ##############\n",
      "0.6336320506062203\n",
      "(1897, 2) (1897,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "392eab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6557722720084344}\n",
      "{'precision': 0.5964063863648031}\n",
      "{'recall': 0.5695046573314416}\n",
      "{'f1': 0.5667236444516265}\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "print(accuracy_metric.compute(predictions=preds, references=predictions.label_ids))\n",
    "print(precision_metric.compute(predictions=preds, references=predictions.label_ids, average='macro'))\n",
    "print(recall_metric.compute(predictions=preds, references=predictions.label_ids, average='macro'))\n",
    "print(f1_metric.compute(predictions=preds, references=predictions.label_ids, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5fe8317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
